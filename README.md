# ðŸ§  Patent Pipeline

A modular **OCR â†’ LLM extraction pipeline** for historical patent documents.
It turns raw scanned PDFs into structured JSONL metadata using Tesseract-based OCR and a local (or cloud) LLM.

---

## Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Requirements](#requirements)
- [Installation](#installation)
- [Data Folders](#data-folders)
- [Configuration](#configuration)
- [Run the Pipeline](#run-the-pipeline)
- [Limiting to a Few Documents](#limiting-to-a-few-documents)
- [Models & Backends](#models--backends)
- [Output Format](#output-format)
- [Prompting Notes](#prompting-notes)
- [Troubleshooting](#troubleshooting)
- [Roadmap](#roadmap)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## Overview

This project automates the extraction of bibliographic fields from patent PDFs:

- **identifier** (e.g., `CH-117732`)
- **title** (keep original language)
- **inventors** (string; multiple separated by `; `)
- **assignee**
- **pub_date_application**, **pub_date_publication**, **pub_date_foreign** (ISO `YYYY-MM-DD` or `null`)
- **address** (English, e.g., `Kiruna (Sweden)`)
- **industrial_field** (short English category)

**Pipeline steps:**

1. **OCR**: Convert PDFs to text via `pdf2image` + `pytesseract` (+ OpenCV preprocessing).
2. **Extraction**: Feed OCR text to an LLM (HF/Transformers or MLX on Apple Silicon) with a strict JSON prompt.
3. **Validation**: Parse and validate with Pydantic models.
4. **Output**: Save one JSON object per document (JSON Lines).

---

## Project Structure

```
patent_pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pdf/           # input PDFs
â”‚   â”œâ”€â”€ ocr_text/          # OCRâ€™d .txt
â”‚   â”œâ”€â”€ predictions/       # JSONL predictions
â”‚   â””â”€â”€ ocr_report.csv     # OCR run report
â”œâ”€â”€ src/patent_pipeline/
â”‚   â”œâ”€â”€ patent_ocr/
â”‚   â”‚   â””â”€â”€ ocr_utils.py               # OCR batch + preprocessing
â”‚   â”œâ”€â”€ pydantic/
â”‚   â”‚   â”œâ”€â”€ hf_agent.py                # model loader (HF/MLX) + JSON extraction
â”‚   â”‚   â”œâ”€â”€ models.py                  # PatentMetadata / PatentExtraction
â”‚   â”‚   â””â”€â”€ prompt_templates.py        # extraction prompt(s)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ device_utils.py            # device detection/logging
â”‚   â”œâ”€â”€ pipeline.py                    # end-to-end runner (OCR â†’ LLM â†’ JSONL)
â”‚   â””â”€â”€ __main__.py                    # allows `python -m patent_pipeline`
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_local.sh                   # local run (Mac / MPS / MLX)
â”‚   â””â”€â”€ run_azure.sh                   # GPU VM run (CUDA)
â”œâ”€â”€ pyproject.toml                     # Poetry deps
â””â”€â”€ README.md
```

---

## Requirements

- **Python** â‰¥ 3.12 (Poetry-managed)
- **macOS (Apple Silicon)** or Linux
- OCR utilities:
  - **Tesseract**
  - **Poppler**
- Optional (Apple Silicon):
  - **MLX** backend (`mlx-lm`)

---

## Installation

```bash
git clone https://github.com/Just-PH/patent_pipeline.git
cd patent_pipeline

poetry install

# OCR deps
brew install tesseract poppler
# or
sudo apt install tesseract-ocr poppler-utils

poetry add sentencepiece
poetry add mlx-lm  # if using MLX backend
```

---

## Run the Pipeline

```bash
export HF_MODEL="mlx-community/Mistral-7B-Instruct-v0.3-8bit" # You can change the mode
export DEVICE="mps" # You can chance the device
bash scripts/run_local.sh
```

---

## License

MIT Â© 2025 â€” Pierre-Henri Delville
