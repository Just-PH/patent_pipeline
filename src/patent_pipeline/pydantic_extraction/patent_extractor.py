# üìÑ src/patent_pipeline/pydantic/patent_extractor.py
from pathlib import Path
import traceback
import os
import json
import regex as re
from typing import Optional, Literal
from pydantic import ValidationError
from .models import PatentExtraction, PatentMetadata
from .prompt_templates import PROMPT_EXTRACTION_V2
from ..utils.device_utils import get_device
from tqdm import tqdm

# Optional deps
try:
    import mlx_lm
    _HAS_MLX = True
except ImportError:
    _HAS_MLX = False

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline


class PatentExtractor:
    """
    Extracteur de m√©tadonn√©es de brevets √† partir de textes OCR.

    Supporte MLX (Apple Silicon) et PyTorch (CPU/CUDA).
    Utilise un LLM pour extraire des champs structur√©s via Pydantic.
    """

    def __init__(
        self,
        model_name: Optional[str] = None,
        backend: Literal["auto", "mlx", "pytorch"] = "auto",
        prompt_template: Optional[str] = None,
        max_ocr_chars: int = 10000,
        max_new_tokens: int = 1024,
        temperature: float = 0.0,
        do_sample: bool = False,
        device: Optional[str] = None,
    ):
        """
        Initialise l'extracteur avec un mod√®le LLM.

        Args:
            model_name: Nom du mod√®le HuggingFace (d√©faut: env HF_MODEL ou Mistral-7B)
            backend: "mlx" (Mac), "pytorch" (CPU/CUDA), ou "auto" (d√©tection)
            prompt_template: Template de prompt personnalis√© (d√©faut: PROMPT_EXTRACTION_V2)
            max_ocr_chars: Nombre max de caract√®res OCR √† envoyer au mod√®le
            max_new_tokens: Tokens max g√©n√©r√©s par le mod√®le
            temperature: Temp√©rature de g√©n√©ration (0 = d√©terministe)
            do_sample: Active le sampling (False pour reproductibilit√©)
            device: Device PyTorch ('cpu', 'cuda', 'mps'), auto-d√©tect√© si None
        """
        self.model_name = model_name or os.getenv("HF_MODEL", "mlx-community/Mistral-7B-Instruct-v0.3")
        self.prompt_template = prompt_template or PROMPT_EXTRACTION_V2
        self.max_ocr_chars = max_ocr_chars
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.do_sample = do_sample

        # D√©tection du backend
        if backend == "auto":
            self.backend = "mlx" if _HAS_MLX else "pytorch"
        else:
            self.backend = backend
            if backend == "mlx" and not _HAS_MLX:
                raise ImportError("MLX n'est pas install√©. Installe avec: pip install mlx-lm")

        # D√©tection du device pour PyTorch
        self.device = device or get_device()
        if self.backend == "pytorch" and self.device == "mps":
            print("‚ö†Ô∏è  MPS backend instable ‚Üí fallback CPU")
            self.device = "cpu"

        # Chargement du mod√®le
        self._load_model()

    def _load_model(self):
        """Charge le mod√®le selon le backend configur√©."""
        print(f"üß† Backend: {self.backend}")
        print(f"üì¶ Model: {self.model_name}")

        if self.backend == "mlx":
            print("‚öôÔ∏è  Loading via MLX (quantized int4/int8)")
            self.model, self.tokenizer = mlx_lm.load(self.model_name)
            self.pipe = None

        elif self.backend == "pytorch":
            # Config PyTorch pour stabilit√© MPS
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"

            dtype = torch.float16 if self.device == "cuda" else torch.float32
            map_arg = self.device if self.device == "cuda" else "cpu"

            print(f"üöÄ Loading on {self.device} ({dtype})")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=dtype,
                device_map=map_arg
            )
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_new_tokens=self.max_new_tokens,
                temperature=self.temperature,
                do_sample=self.do_sample
            )

    def set_prompt_template(self, template: str):
        """
        Change le template de prompt utilis√© pour l'extraction.

        Args:
            template: Nouveau template avec placeholder {text}
        """
        if "{text}" not in template:
            raise ValueError("Le template doit contenir le placeholder {text}")
        self.prompt_template = template

    def _truncate_ocr(self, text: str) -> str:
        """Tronque le texte OCR si trop long."""
        if len(text) > self.max_ocr_chars:
            return text[:self.max_ocr_chars] + "\n[...] (truncated)"
        return text

    def _generate(self, prompt: str) -> str:
        """
        G√©n√®re du texte avec le mod√®le charg√©.

        Args:
            prompt: Prompt complet √† envoyer au mod√®le

        Returns:
            Texte g√©n√©r√© brut
        """
        if self.backend == "mlx":
            output = mlx_lm.generate(
                self.model,
                self.tokenizer,
                prompt,
                max_tokens=self.max_new_tokens
            )
            return output.strip()

        elif self.backend == "pytorch":
            out = self.pipe(prompt)[0]
            return out.get("generated_text") or out.get("text") or ""

    def _extract_json(self, text: str) -> str:
        """
        Extrait le premier bloc JSON valide du texte g√©n√©r√©.

        Args:
            text: Texte brut du mod√®le

        Returns:
            Cha√Æne JSON extraite (ou "{}" si √©chec)
        """
        # Recherche du bloc {...} complet
        m = re.search(r'\{(?:[^{}]|(?R))*\}', text, re.DOTALL)
        if m:
            return m.group(0)

        # Fallback: recherche partielle √† partir de "identifier"
        alt = re.search(r'"identifier".*', text, re.DOTALL)
        if alt:
            raw = alt.group(0).strip()
            if not raw.startswith("{"):
                raw = "{\n" + raw
            if not raw.endswith("}"):
                raw += "\n}"
            return raw

        return "{}"

    def _normalize_entity_list(self, value):
        """
        Normalise une liste d'entit√©s (inventeurs/assignees).

        Accepte:
        - Liste de dicts [{"name": ..., "address": ...}]
        - String "Jean Dupont (Paris); Marie Curie (Versailles)"

        Returns:
            Liste de dicts normalis√©s ou None
        """
        if value is None:
            return None

        # D√©j√† au bon format
        if isinstance(value, list) and all(isinstance(x, dict) for x in value):
            return value

        # Parsing depuis string
        if isinstance(value, str):
            entities = []
            for chunk in value.split(";"):
                chunk = chunk.strip()
                if not chunk:
                    continue
                # Extraction "Nom (Ville)"
                m = re.match(r"(.+?)\s*\(([^)]+)\)", chunk)
                if m:
                    entities.append({
                        "name": m.group(1).strip(),
                        "address": m.group(2).strip()
                    })
                else:
                    entities.append({"name": chunk, "address": None})
            return entities if entities else None

        return None

    def _is_company_name(self, name: str) -> bool:
        """
        D√©tecte si un nom est probablement une entreprise.

        Indices :
        - Contient &, und, et
        - Contient GmbH, AG, SA, Co., KG, Ltd, Inc
        - Tout en majuscules (>= 50% de lettres majuscules)

        Args:
            name: Nom √† analyser

        Returns:
            True si c'est probablement une entreprise
        """
        if not name:
            return False

        name_lower = name.lower()

        # Patterns √©vidents de compagnies
        company_patterns = [
            r'\&',  # &
            r'\bund\b',  # und
            r'\bet\b',  # et
            r'\bgmbh\b',
            r'\bag\b',
            r'\bsa\b',
            r'\bco\.',
            r'\bkg\b',
            r'\bltd\b',
            r'\binc\b',
            r'\bcorp\b',
            r'\bs\.a\.',
            r'\bs\.r\.l\.',
        ]

        for pattern in company_patterns:
            if re.search(pattern, name_lower):
                return True

        # Heuristique : beaucoup de majuscules = entreprise
        letters = [c for c in name if c.isalpha()]
        if letters:
            upper_ratio = sum(1 for c in letters if c.isupper()) / len(letters)
            if upper_ratio > 0.6:  # Plus de 60% de majuscules
                return True

        return False

    def _fix_inventor_assignee_confusion(self, data: dict) -> dict:
        """
        Corrige automatiquement les confusions inventors/assignees.

        Si des noms d'entreprise sont dans inventors, les d√©place vers assignees.

        Args:
            data: Dict avec champs inventors et assignees

        Returns:
            Dict corrig√©
        """
        inventors = data.get("inventors") or []
        assignees = data.get("assignees") or []

        if not inventors:
            return data

        # S√©parer vrais inventors et companies mal plac√©es
        true_inventors = []
        misplaced_companies = []

        for inventor in inventors:
            if isinstance(inventor, dict):
                name = inventor.get("name", "")
                if self._is_company_name(name):
                    misplaced_companies.append(inventor)
                else:
                    true_inventors.append(inventor)

        # Si on a trouv√© des compagnies mal plac√©es
        if misplaced_companies:
            print(f"üîß Correction : {len(misplaced_companies)} entreprise(s) d√©plac√©e(s) vers assignees")
            for company in misplaced_companies:
                print(f"   ‚Üí {company.get('name')}")

            # Fusionner avec les assignees existants
            all_assignees = assignees + misplaced_companies

            data["inventors"] = true_inventors if true_inventors else None
            data["assignees"] = all_assignees if all_assignees else None

        return data

    def _fix_duplicate_dates(self, data: dict) -> dict:
        """
        Corrige les dates dupliqu√©es.

        Si pub_date_application == pub_date_publication et qu'il n'y a pas de foreign date,
        on assume que c'est la date de publication, pas d'application.

        Logique :
        - Si les 3 dates sont identiques ‚Üí garder seulement publication
        - Si application == publication (mais ‚â† foreign) ‚Üí mettre application √† None
        - Si une seule date existe ‚Üí c'est probablement la publication

        Args:
            data: Dict avec champs de dates

        Returns:
            Dict corrig√©
        """
        app_date = data.get("pub_date_application")
        pub_date = data.get("pub_date_publication")
        foreign_date = data.get("pub_date_foreign")

        # Cas 1 : Application == Publication (duplication probable)
        if app_date and pub_date and app_date == pub_date:
            # Si foreign est diff√©rent, on garde les 3
            if foreign_date and foreign_date != app_date:
                print(f"üîß Correction dates : application et publication identiques ({app_date})")
                print(f"   ‚Üí Interpr√©tation : {app_date} = publication (car foreign={foreign_date} existe)")
                data["pub_date_application"] = None
            else:
                # Pas de foreign ou foreign identique aussi ‚Üí c'est juste la publication
                print(f"üîß Correction dates : une seule date trouv√©e ({app_date})")
                print(f"   ‚Üí Interpr√©tation : {app_date} = date de publication")
                data["pub_date_application"] = None

        # Cas 2 : Les 3 dates identiques (tr√®s improbable)
        if app_date and pub_date and foreign_date and app_date == pub_date == foreign_date:
            print(f"üîß Correction dates : 3 dates identiques ({app_date})")
            print(f"   ‚Üí Garde seulement publication")
            data["pub_date_application"] = None
            data["pub_date_foreign"] = None

        return data

    def _parse_and_validate(self, json_str: str) -> PatentMetadata:
        """
        Parse le JSON et valide avec Pydantic.

        Args:
            json_str: Cha√Æne JSON brute

        Returns:
            PatentMetadata valid√©
        """
        try:
            data = json.loads(json_str)

            # Gestion des types inattendus
            if not isinstance(data, dict):
                print(f"‚ö†Ô∏è JSON type inattendu: {type(data)}")
                data = data[0] if isinstance(data, list) and data else {}

            # R√©trocompatibilit√© des noms de champs
            if "assignee" in data and "assignees" not in data:
                data["assignees"] = data.pop("assignee")
            if "inventor" in data and "inventors" not in data:
                data["inventors"] = data.pop("inventor")
            if "class" in data and "classification" not in data:
                data["classification"] = data.pop("class")

            # Initialisation des champs requis
            required_fields = [
                "title", "inventors", "assignees",
                "pub_date_application", "pub_date_publication", "pub_date_foreign",
                "classification", "industrial_field"
            ]
            for key in required_fields:
                data.setdefault(key, None)

            # Normalisation des listes
            data["inventors"] = self._normalize_entity_list(data.get("inventors"))
            data["assignees"] = self._normalize_entity_list(data.get("assignees"))

            # üîß CORRECTION AUTOMATIQUE : d√©placer les entreprises mal class√©es
            data = self._fix_inventor_assignee_confusion(data)

            # üîß CORRECTION AUTOMATIQUE : g√©rer les dates dupliqu√©es
            data = self._fix_duplicate_dates(data)

            return PatentMetadata(**data)

        except (json.JSONDecodeError, ValidationError, KeyError) as e:
            print(f"‚ö†Ô∏è  Erreur de validation JSON: {e}")
            print(f"‚Üí JSON brut:\n{json_str}\n")
            return PatentMetadata(identifier="unknown")

    def extract(self, ocr_text: str, debug: bool = False) -> PatentExtraction:
        """
        Extrait les m√©tadonn√©es structur√©es d'un texte OCR.

        Args:
            ocr_text: Texte brut issu de l'OCR
            debug: Si True, affiche le prompt complet et la sortie brute

        Returns:
            PatentExtraction avec metadata Pydantic valid√©
        """
        # Troncature si n√©cessaire
        truncated_text = self._truncate_ocr(ocr_text)

        # Construction du prompt
        prompt = self.prompt_template.format(text=truncated_text)
        prompt += "\n\nNow output ONLY the JSON object, without any extra text.\n"

        if debug:
            print("=" * 80)
            print("üìù PROMPT ENVOY√â AU MOD√àLE:")
            print("=" * 80)
            print(prompt)
            print("=" * 80)

        # G√©n√©ration
        raw_output = self._generate(prompt)

        if debug:
            print("\n" + "=" * 80)
            print("ü§ñ SORTIE BRUTE DU MOD√àLE:")
            print("=" * 80)
            print(raw_output)
            print("=" * 80)

        # Extraction et parsing JSON
        json_str = self._extract_json(raw_output)

        if debug:
            print("\n" + "=" * 80)
            print("üì¶ JSON EXTRAIT:")
            print("=" * 80)
            print(json_str)
            print("=" * 80 + "\n")

        metadata = self._parse_and_validate(json_str)

        return PatentExtraction(
            ocr_text=ocr_text,
            model=self.model_name,
            prediction=metadata
        )

    def extract_from_file(self, txt_path: Path) -> dict:
        """
        Extrait les m√©tadonn√©es d'un fichier .txt.

        Args:
            txt_path: Chemin vers le fichier OCR .txt

        Returns:
            Dict s√©rialisable en JSON (pr√™t pour JSONL)
        """
        try:
            ocr_text = txt_path.read_text(encoding="utf-8")

            if not ocr_text.strip():
                return {
                    "file_name": txt_path.name,
                    "ocr_path": str(txt_path),
                    "error": "empty_ocr"
                }

            extraction = self.extract(ocr_text)
            record = extraction.model_dump(mode="json")
            record["file_name"] = txt_path.name
            record["ocr_path"] = str(txt_path)

            # Extraction de l'identifier depuis le nom du fichier
            record["prediction"]["identifier"] = txt_path.stem.split("_")[0]

            return record

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur sur {txt_path.name}: {e}")
            traceback.print_exc()
            return {
                "file_name": txt_path.name,
                "ocr_path": str(txt_path),
                "error": f"exception: {e.__class__.__name__}"
            }

    def batch_extract(
        self,
        txt_dir: Path,
        out_file: Path,
        limit: Optional[int] = None
    ) -> int:
        """
        Traite un dossier de fichiers .txt en batch.

        Args:
            txt_dir: Dossier contenant les fichiers .txt
            out_file: Fichier JSONL de sortie
            limit: Nombre max de fichiers √† traiter (None = tous)

        Returns:
            Nombre de documents trait√©s avec succ√®s
        """
        txt_files = sorted(txt_dir.glob("*.txt"))
        total = len(txt_files)

        if limit is not None and limit < total:
            txt_files = txt_files[:limit]
            print(f"‚öôÔ∏è Limitation √† {limit} documents (sur {total} total)")
        else:
            print(f"‚öôÔ∏è Traitement de {total} documents")

        out_file.parent.mkdir(parents=True, exist_ok=True)
        count = 0

        with open(out_file, "w", encoding="utf-8") as f_out:
            for txt_path in tqdm(txt_files, desc="üß† Batch extraction", unit="doc"):
                try:
                    record = self.extract_from_file(txt_path)
                    f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
                    count += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur sur {txt_path.name}: {e}")

        print(f"‚úÖ Extraction compl√®te ‚Üí {count} documents trait√©s")
        print(f"üìä R√©sultats: {out_file}")

        return count
